# ä½¿ç”¨å¸¸ç”¨å­—è¡¨é€²è¡Œ Fine-tune æŒ‡å—

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è¿°

åªä½¿ç”¨æ•™è‚²éƒ¨ 4808 å€‹å¸¸ç”¨å­—é€²è¡Œè¨“ç·´ï¼Œå¯ä»¥ï¼š
- âœ… **æ¸›å°‘è¨“ç·´æ™‚é–“**ï¼šæ•¸æ“šé‡æ›´å°‘ï¼Œè¨“ç·´æ›´å¿«
- âœ… **æé«˜æº–ç¢ºç‡**ï¼šå°ˆæ³¨æ–¼å¸¸ç”¨å­—ï¼Œé¿å…ç”Ÿåƒ»å­—å¹²æ“¾
- âœ… **æ›´å¯¦ç”¨**ï¼š4808 å€‹å¸¸ç”¨å­—å·²ç¶“èƒ½è¦†è“‹ 99% çš„æ—¥å¸¸ä½¿ç”¨å ´æ™¯

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ­¥é©Ÿ 1ï¼šå®‰è£ä¾è³´

é¦–å…ˆéœ€è¦å®‰è£ pandas å’Œ openpyxl ä¾†è®€å– Excel æ–‡ä»¶ï¼š

```bash
pip install pandas openpyxl
```

### æ­¥é©Ÿ 2ï¼šä¿®æ”¹è½‰æ›è…³æœ¬

ç·¨è¼¯ `convert_with_common_chars.py`ï¼Œä¿®æ”¹ `main()` å‡½æ•¸ä¸­çš„è·¯å¾‘ï¼š

```python
def main():
    # ä¿®æ”¹é€™äº›è·¯å¾‘
    binarized_data_dir = "/your/path/to/binarized_data"
    samples_csv_path = "/your/path/to/finetune_data/samples.csv"
    id2char_json_path = "/your/path/to/finetune_data/id2char.json"

    # å¸¸ç”¨å­—è¡¨è·¯å¾‘
    common_chars_path = "/your/path/to/finetune_data/æ•™è‚²éƒ¨4808å€‹å¸¸ç”¨å­—.xls"

    output_dir = "./train_data"
    train_ratio = 0.9
    min_samples_per_char = 5  # æ¯å€‹å­—ç¬¦è‡³å°‘ 5 å€‹æ¨£æœ¬
    max_samples_per_char = 200  # æ¯å€‹å­—ç¬¦æœ€å¤š 200 å€‹æ¨£æœ¬ï¼ˆé¿å…æ•¸æ“šä¸å¹³è¡¡ï¼‰
```

### æ­¥é©Ÿ 3ï¼šé‹è¡Œè½‰æ›

```bash
python convert_with_common_chars.py
```

### æ­¥é©Ÿ 4ï¼šæª¢æŸ¥çµæœ

```bash
# æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
ls train_data/

# æŸ¥çœ‹è¨“ç·´æ¨£æœ¬
head train_data/train_list.txt

# æŸ¥çœ‹å­—å…¸ï¼ˆæ‡‰è©²åªåŒ…å«å¸¸ç”¨å­—ï¼‰
head train_data/custom_dict.txt
wc -l train_data/custom_dict.txt  # æ‡‰è©²ä¸è¶…é 4808 å€‹å­—ç¬¦

# æŸ¥çœ‹çµ±è¨ˆä¿¡æ¯
cat train_data/dataset_stats.json
```

### æ­¥é©Ÿ 5ï¼šé–‹å§‹è¨“ç·´

```bash
python tools/train.py \
    -c configs/rec/PP-OCRv5/PP-OCRv5_mobile_rec.yml \
    -o Global.pretrained_model=./pretrained/PP-OCRv5_mobile_rec_pretrained \
       Global.character_dict_path=./train_data/custom_dict.txt \
       Train.dataset.label_file_list=['./train_data/train_list.txt'] \
       Eval.dataset.label_file_list=['./train_data/val_list.txt'] \
       Optimizer.lr.learning_rate=0.00005
```

---

## ğŸ”§ é€²éšé…ç½®

### 1. èª¿æ•´æ¯å€‹å­—ç¬¦çš„æ¨£æœ¬æ•¸

å¦‚æœä½ çš„æ•¸æ“šåˆ†ä½ˆä¸å‡ï¼ˆæœ‰äº›å­—å¾ˆå¤šæ¨£æœ¬ï¼Œæœ‰äº›å­—å¾ˆå°‘ï¼‰ï¼Œå¯ä»¥èª¿æ•´ï¼š

```python
# åœ¨ convert_with_common_chars.py ä¸­ä¿®æ”¹
min_samples_per_char = 10   # æé«˜æœ€ä½è¦æ±‚ï¼Œéæ¿¾æ‰æ¨£æœ¬å¤ªå°‘çš„å­—
max_samples_per_char = 100  # é™ä½ä¸Šé™ï¼Œè®“æ•¸æ“šæ›´å¹³è¡¡
```

**æ•ˆæœï¼š**
- `min_samples_per_char` è¶Šé«˜ï¼šå­—ç¬¦æ•¸è¶Šå°‘ï¼Œä½†æ¯å€‹å­—ç¬¦çš„è³ªé‡æ›´å¥½
- `max_samples_per_char` è¶Šä½ï¼šæ•¸æ“šè¶Šå¹³è¡¡ï¼Œä½†å¯èƒ½æµªè²»ä¸€äº›å„ªè³ªæ•¸æ“š

**å»ºè­°é…ç½®ï¼š**
- æ•¸æ“šå……è¶³ï¼ˆç¸½æ¨£æœ¬ > 100è¬ï¼‰ï¼š`min=10, max=100`
- æ•¸æ“šä¸­ç­‰ï¼ˆç¸½æ¨£æœ¬ 10-100è¬ï¼‰ï¼š`min=5, max=200`
- æ•¸æ“šè¼ƒå°‘ï¼ˆç¸½æ¨£æœ¬ < 10è¬ï¼‰ï¼š`min=3, max=None`ï¼ˆä¸é™åˆ¶ï¼‰

### 2. èª¿æ•´è¨“ç·´/é©—è­‰é›†æ¯”ä¾‹

```python
train_ratio = 0.95  # 95% è¨“ç·´ï¼Œ5% é©—è­‰ï¼ˆæ•¸æ“šå¤šæ™‚ï¼‰
# æˆ–
train_ratio = 0.85  # 85% è¨“ç·´ï¼Œ15% é©—è­‰ï¼ˆæ•¸æ“šå°‘æ™‚ï¼Œéœ€è¦æ›´å¤šé©—è­‰é›†ï¼‰
```

---

## ğŸ†˜ å‚™ç”¨æ–¹æ¡ˆï¼šExcel è®€å–å¤±æ•—

å¦‚æœè…³æœ¬ç„¡æ³•è®€å– Excel æ–‡ä»¶ï¼Œå¯ä»¥æ‰‹å‹•è½‰æ›ç‚º TXT æ ¼å¼ï¼š

### æ–¹æ³• 1ï¼šä½¿ç”¨ Excel æ‰‹å‹•è½‰æ›

1. æ‰“é–‹ `æ•™è‚²éƒ¨4808å€‹å¸¸ç”¨å­—.xls`
2. è¤‡è£½æ‰€æœ‰å¸¸ç”¨å­—
3. ç²˜è²¼åˆ°æ–°çš„æ–‡æœ¬æ–‡ä»¶ `common_chars.txt`
4. æ¯è¡Œä¸€å€‹å­—ï¼Œæˆ–è€…ä¸€è¡Œå…¨éƒ¨å­—ç¬¦éƒ½å¯ä»¥

ç¯„ä¾‹ `common_chars.txt`ï¼š
```
ä¸€
äºŒ
ä¸‰
...
```

æˆ–è€…ï¼š
```
ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å„„...
```

### æ–¹æ³• 2ï¼šä½¿ç”¨ Python è…³æœ¬è½‰æ›

å‰µå»ºä¸€å€‹ç°¡å–®çš„è½‰æ›è…³æœ¬ `convert_excel_to_txt.py`ï¼š

```python
import pandas as pd

# è®€å– Excel
df = pd.read_excel('finetune_data/æ•™è‚²éƒ¨4808å€‹å¸¸ç”¨å­—.xls')

# æå–å­—ç¬¦ï¼ˆå‡è¨­åœ¨ç¬¬ä¸€åˆ—ï¼‰
chars = df.iloc[:, 0].tolist()

# ä¿å­˜ç‚ºæ–‡æœ¬æ–‡ä»¶
with open('common_chars.txt', 'w', encoding='utf-8') as f:
    for char in chars:
        if str(char) != 'nan' and len(str(char)) > 0:
            f.write(str(char)[0] + '\n')

print(f"å·²ä¿å­˜ {len(chars)} å€‹å¸¸ç”¨å­—åˆ° common_chars.txt")
```

é‹è¡Œï¼š
```bash
pip install pandas openpyxl
python convert_excel_to_txt.py
```

### æ–¹æ³• 3ï¼šç›´æ¥æä¾›å¸¸ç”¨å­—åˆ—è¡¨

å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½ä¸è¡Œï¼Œæˆ‘å¯ä»¥å¹«ä½ æº–å‚™ä¸€å€‹å¸¸ç”¨å­—æ–‡æœ¬æ–‡ä»¶ã€‚æ•™è‚²éƒ¨å¸¸ç”¨å­—é€šå¸¸åŒ…æ‹¬ï¼š

```python
# å‰µå»º common_chars.txt
common_chars = """
çš„ä¸€æ˜¯ä¸äº†äººæˆ‘åœ¨æœ‰ä»–é€™ç‚ºä¹‹å¤§ä¾†ä»¥å€‹ä¸­ä¸Šå€‘åˆ°èªªåœ‹å’Œåœ°ä¹Ÿå­æ™‚é“å‡ºè€Œè¦æ–¼å°±ä¸‹å¾—å¯ä½ å¹´ç”Ÿè‡ªæœƒé‚£å¾Œèƒ½å°è‘—äº‹å…¶è£¡æ‰€å»è¡Œéå®¶åç”¨ç™¼å¤©å¦‚ç„¶ä½œæ–¹æˆè€…å¤šæ—¥éƒ½ä¸‰å°è»äºŒç„¡åŒéº¼ç¶“æ³•ç•¶èµ·èˆ‡å¥½çœ‹å­¸é€²ç¨®å°‡é‚„åˆ†æ­¤å¿ƒå‰é¢åˆå®šè¦‹åªä¸»æ²’å…¬å¾...
"""

with open('common_chars.txt', 'w', encoding='utf-8') as f:
    for char in common_chars:
        if not char.isspace():
            f.write(char + '\n')
```

ç„¶å¾Œä¿®æ”¹è…³æœ¬ä½¿ç”¨ TXT æ–‡ä»¶ï¼š
```python
common_chars_path = "/your/path/to/common_chars.txt"
```

---

## ğŸ“Š é æœŸæ•ˆæœ

### æ•¸æ“šçµ±è¨ˆç¯„ä¾‹

å‡è¨­ä½ çš„åŸå§‹æ•¸æ“šæœ‰ï¼š
- ç¸½æ¨£æœ¬æ•¸ï¼š1,000,000
- å­—ç¬¦ç¨®é¡ï¼š13,000+ å€‹ï¼ˆåŒ…å«å¾ˆå¤šç”Ÿåƒ»å­—ï¼‰

ä½¿ç”¨å¸¸ç”¨å­—éæ¿¾å¾Œï¼š
- ç¸½æ¨£æœ¬æ•¸ï¼š~800,000ï¼ˆ80%ï¼‰
- å­—ç¬¦ç¨®é¡ï¼š~4,000 å€‹ï¼ˆæ•™è‚²éƒ¨å¸¸ç”¨å­—ï¼‰
- æ¯å­—ç¬¦å¹³å‡ï¼š200 å€‹æ¨£æœ¬

### è¨“ç·´æ™‚é–“ä¼°ç®—

- **åŸå§‹æ•¸æ“š**ï¼ˆ13,000 å­—ç¬¦ï¼‰ï¼š~6-8 å°æ™‚ï¼ˆå–®å¡ï¼‰
- **å¸¸ç”¨å­—æ•¸æ“š**ï¼ˆ4,000 å­—ç¬¦ï¼‰ï¼š~2-3 å°æ™‚ï¼ˆå–®å¡ï¼‰

### æº–ç¢ºç‡é æœŸ

- å¸¸ç”¨å­—æº–ç¢ºç‡ï¼š**95%+**
- è¨“ç·´é€Ÿåº¦ï¼š**å¿« 2-3 å€**
- æ¨ç†é€Ÿåº¦ï¼š**ç•¥å¿«**ï¼ˆå­—å…¸æ›´å°ï¼‰

---

## ğŸ¯ å¸¸è¦‹å•é¡Œ

### Q1: å¦‚æœæŸäº›å¸¸ç”¨å­—åœ¨æˆ‘çš„æ•¸æ“šé›†ä¸­æ²’æœ‰æ€éº¼è¾¦ï¼Ÿ

**A:** è…³æœ¬æœƒè‡ªå‹•éæ¿¾ï¼Œæœ€çµ‚å­—å…¸åªåŒ…å«**å¯¦éš›æœ‰æ•¸æ“šçš„å¸¸ç”¨å­—**ã€‚æ¯”å¦‚æ•™è‚²éƒ¨åˆ—è¡¨æœ‰ 4808 å€‹å­—ï¼Œä½†ä½ çš„æ•¸æ“šåªæœ‰ 3500 å€‹å¸¸ç”¨å­—æœ‰æ¨£æœ¬ï¼Œé‚£å­—å…¸å°±åªæœƒåŒ…å«é€™ 3500 å€‹ã€‚

### Q2: æˆ‘æ‡‰è©²éæ¿¾æ‰æ¨£æœ¬æ•¸å¤ªå°‘çš„å­—ç¬¦å—ï¼Ÿ

**A:** å»ºè­°éæ¿¾ã€‚å¦‚æœæŸå€‹å­—ç¬¦åªæœ‰ 1-2 å€‹æ¨£æœ¬ï¼Œæ¨¡å‹å¾ˆé›£å­¸å¥½ã€‚æ¨è–¦è¨­ç½®ï¼š
```python
min_samples_per_char = 5  # è‡³å°‘ 5 å€‹æ¨£æœ¬
```

### Q3: æˆ‘çš„æ•¸æ“šä¸å¹³è¡¡ï¼Œæœ‰äº›å­—æœ‰ 1000 å€‹æ¨£æœ¬ï¼Œæœ‰äº›åªæœ‰ 10 å€‹ï¼Ÿ

**A:** ä½¿ç”¨ `max_samples_per_char` é™åˆ¶ï¼š
```python
max_samples_per_char = 200  # æ¯å€‹å­—ç¬¦æœ€å¤š 200 å€‹æ¨£æœ¬
```

é€™æœƒè®“æ¨¡å‹å°æ¯å€‹å­—ç¬¦çš„å­¸ç¿’æ›´å‡è¡¡ã€‚

### Q4: æˆ‘èƒ½åŒæ™‚ä½¿ç”¨å¸¸ç”¨å­— + éƒ¨åˆ†ç”Ÿåƒ»å­—å—ï¼Ÿ

**A:** å¯ä»¥ï¼ä¿®æ”¹è…³æœ¬ä¸­çš„å¸¸ç”¨å­—åˆ—è¡¨ï¼Œæ·»åŠ ä½ éœ€è¦çš„ç”Ÿåƒ»å­—ï¼š

```python
# è®€å–å¸¸ç”¨å­—
common_chars = read_common_chars_from_excel(common_chars_path)

# æ·»åŠ é¡å¤–çš„ç”Ÿåƒ»å­—
extra_chars = {'ã—Š', 'ã€', 'é¾˜'}  # ä½ éœ€è¦çš„ç”Ÿåƒ»å­—
common_chars.update(extra_chars)
```

---

## ğŸ“ å®Œæ•´æµç¨‹æª¢æŸ¥æ¸…å–®

- [ ] å·²å®‰è£ `pandas` å’Œ `openpyxl`
- [ ] å·²æº–å‚™å¥½æ•™è‚²éƒ¨å¸¸ç”¨å­—è¡¨ï¼ˆ.xls æˆ– .txtï¼‰
- [ ] å·²ä¿®æ”¹ `convert_with_common_chars.py` ä¸­çš„è·¯å¾‘
- [ ] å·²è¨­ç½®åˆé©çš„ `min_samples_per_char` å’Œ `max_samples_per_char`
- [ ] é‹è¡Œè½‰æ›è…³æœ¬æˆåŠŸ
- [ ] æª¢æŸ¥ç”Ÿæˆçš„ `custom_dict.txt` å­—ç¬¦æ•¸åˆç†ï¼ˆ~3000-4500ï¼‰
- [ ] æª¢æŸ¥ `dataset_stats.json` ä¸­çš„çµ±è¨ˆä¿¡æ¯
- [ ] æŸ¥çœ‹ `train_list.txt` å‰å¹¾è¡Œç¢ºèªæ ¼å¼æ­£ç¢º
- [ ] ä¸‹è¼‰ PP-OCRv5 é è¨“ç·´æ¨¡å‹
- [ ] é–‹å§‹è¨“ç·´ï¼

---

## ğŸ’¡ å¿«é€Ÿå‘½ä»¤ï¼ˆè¤‡è£½å³ç”¨ï¼‰

```bash
# 1. å®‰è£ä¾è³´
pip install pandas openpyxl

# 2. ä¿®æ”¹è…³æœ¬å¾Œé‹è¡Œè½‰æ›
python convert_with_common_chars.py

# 3. æª¢æŸ¥çµæœ
head train_data/train_list.txt
wc -l train_data/custom_dict.txt

# 4. ä¸‹è¼‰é è¨“ç·´æ¨¡å‹ï¼ˆå¦‚æœé‚„æ²’ä¸‹è¼‰ï¼‰
mkdir -p pretrained && cd pretrained
wget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv5_mobile_rec_pretrained.pdparams
cd ..

# 5. é–‹å§‹è¨“ç·´
python tools/train.py \
    -c configs/rec/PP-OCRv5/PP-OCRv5_mobile_rec.yml \
    -o Global.pretrained_model=./pretrained/PP-OCRv5_mobile_rec_pretrained \
       Global.character_dict_path=./train_data/custom_dict.txt \
       Global.save_model_dir=./output/common_chars_finetune \
       Train.dataset.label_file_list=['./train_data/train_list.txt'] \
       Eval.dataset.label_file_list=['./train_data/val_list.txt'] \
       Optimizer.lr.learning_rate=0.00005 \
       Train.loader.batch_size_per_card=64

# 6. æŸ¥çœ‹è¨“ç·´æ—¥èªŒ
tail -f output/common_chars_finetune/train.log
```

---

Good luck! ğŸš€
